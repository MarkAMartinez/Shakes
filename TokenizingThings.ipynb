{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import parse_corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"preprocessed_shakespeare.txt\"\n",
    "castfile = \"curated_cast.txt\"\n",
    "\n",
    "sections = parse_corpus.process(filename)\n",
    "castdict = {}\n",
    "for line in open(castfile):\n",
    "    words = nltk.word_tokenize(line.strip())\n",
    "    for word in words:\n",
    "        castdict[word.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"/Users/markmartinez/Desktop/shakespeare.txt\",\"r\") as F:\n",
    "#     allText = F.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords, tokenize, stem (takes about 30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palac enter black deliv buri husband go madam weep death anew must attend majesti command ward evermor subject shall find husband madam sir gener time good must necess hold virtu whose worthi would stir want rather lack abund hope majesti amend hath abandon physician madam whose practic hath persecut time hope find advantag process lose hope time sad passag skill almost great honesti stretch far would made natur immort death play lack work would sake live think would death diseas call speak mada\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stopdict = dict((s.lower(),None) for s in stops) # Sets are really terrible in Python\n",
    "# print [s for s in stopdict.iterkeys()]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "clean_sections = []\n",
    "\n",
    "# NOTE(tfs): I am using tokenization now. Mainly it really cleans up punctuation and handles contractions well\n",
    "for section in sections:\n",
    "#     secwords = section.split()\n",
    "#     tokens = nltk.word_tokenize(\" \".join(secwords))\n",
    "    tokens = nltk.word_tokenize(section)\n",
    "    nonstops = [w for w in tokens if not (w.lower() in stopdict or w.lower() in castdict)]\n",
    "    stemmed = [ps.stem(t.lower()) for t in nonstops if t.isalnum()]\n",
    "#     nonstops = [w for w in stemmed if not (w in stopdict or w in castdict)]\n",
    "    clean_sections.append(\" \".join(stemmed)) # Note this does not preserve structure,\n",
    "                                              #      but all words are now present in the section string\n",
    "print clean_sections[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stops = set(stopwords.words(\"english\"))\n",
    "# words = allText.split()\n",
    "# meaningful_words = [w for w in words if not w in stops]\n",
    "# cleanedUp =  \" \".join( meaningful_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanedUp[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now split on 'scene'\n",
    "# scenes = [x.strip() for x in cleanedUp.split(\"scene\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "data = vectorizer.fit_transform(clean_sections)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = data.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402\n",
      "12740\n",
      "[u'acquit', u'acquitt', u'acr', u'across', u'act', u'actaeon', u'action', u'actium', u'activ', u'actor', u'actual', u'acut', u'ad', u'adag', u'adalla', u'adam', u'add', u'adder', u'addict', u'addit', u'addl', u'address', u'addrest', u'adher', u'adieu', u'adjac', u'adjoin', u'adjourn', u'adjudg', u'adjunct', u'administ', u'administr', u'admir', u'admiringli', u'admit', u'admitt', u'admonish', u'admonit', u'ado', u'adoni', u'adopt', u'adoptedli', u'adopti', u'ador', u'adorest', u'adoreth', u'adorn', u'adramadio', u'adriat', u'adsum', u'adul', u'adulter', u'adulteress', u'adulteri', u'adultress', u'advanc', u'advantag', u'adventur', u'advers', u'adversari', u'adverti', u'advertis', u'advi', u'advic', u'advis', u'advisedli', u'advoc', u'aeacid', u'aeacida', u'aedil', u'aegl', u'aeolu', u'aer', u'aeri', u'aerial', u'aesculapiu', u'aeson', u'aesop', u'aetna', u'afar', u'afear', u'afeard', u'affabl', u'affair', u'affect', u'affecteth', u'affection', u'affeer', u'affi', u'affianc', u'affin', u'affirm', u'afflict', u'afford', u'affordeth', u'affright', u'affront', u'afield', u'afir', u'afloat', u'afoot', u'afor', u'aforehand', u'aforesaid', u'afraid', u'afresh', u'afric', u'africa', u'african', u'afternoon', u'afterward', u'ag', u'agamemmon', u'agat', u'agaz', u'age', u'agenor', u'agent', u'aggrav', u'aggrief', u'agil', u'agincourt', u'agit', u'agniz', u'ago', u'agon', u'agoni', u'agre', u'agreement', u'aground', u'agu', u'aguefac', u'ah', u'ahungri', u'ai', u'aialvolio', u'aiaria', u'aid', u'aidanc', u'aidless', u'ail', u'aim', u'aimest', u'ainsi', u'air', u'airi', u'airless', u'akil', u'ala', u'alabast', u'alack', u'alacr', u'alarm', u'alarum', u'alb', u'alban', u'albeit', u'albion', u'alchemi', u'alchemist', u'alcid', u'aldermen', u'ale', u'alecto', u'alehous', u'alengon', u'aleppo', u'alewif', u'alexand', u'alexandria', u'alexandrian', u'alia', u'alic', u'alien', u'aliena', u'alight', u'alik', u'alisand', u'aliv', u'alla', u'allay', u'alleg', u'allegi', u'alley', u'allhallowma', u'alli', u'allianc', u'allicholi', u'allig', u'allon', u'allot', u'allotteri', u'allow', u'allur', u'allus', u'allycholli', u'alm', u'almain', u'almanac', u'almanack']\n"
     ]
    }
   ],
   "source": [
    "print sum(train_data_features[300])\n",
    "print len(vocab)\n",
    "print vocab[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# vocab = vectorizer.get_feature_names()\n",
    "# print(len(vocab))\n",
    "\n",
    "print type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "\n",
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(data, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run LDA (takes a bit of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "num_topics = 5\n",
    "num_iter = 500\n",
    "n_top_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 643\n",
      "INFO:lda:vocab_size: 12740\n",
      "INFO:lda:n_words: 317704\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -3040766\n",
      "INFO:lda:<10> log likelihood: -2784113\n",
      "INFO:lda:<20> log likelihood: -2697490\n",
      "INFO:lda:<30> log likelihood: -2652065\n",
      "INFO:lda:<40> log likelihood: -2628219\n",
      "INFO:lda:<50> log likelihood: -2612491\n",
      "INFO:lda:<60> log likelihood: -2601757\n",
      "INFO:lda:<70> log likelihood: -2596991\n",
      "INFO:lda:<80> log likelihood: -2594096\n",
      "INFO:lda:<90> log likelihood: -2590554\n",
      "INFO:lda:<100> log likelihood: -2586391\n",
      "INFO:lda:<110> log likelihood: -2582836\n",
      "INFO:lda:<120> log likelihood: -2581377\n",
      "INFO:lda:<130> log likelihood: -2579058\n",
      "INFO:lda:<140> log likelihood: -2578207\n",
      "INFO:lda:<150> log likelihood: -2577863\n",
      "INFO:lda:<160> log likelihood: -2575220\n",
      "INFO:lda:<170> log likelihood: -2576711\n",
      "INFO:lda:<180> log likelihood: -2574336\n",
      "INFO:lda:<190> log likelihood: -2576026\n",
      "INFO:lda:<200> log likelihood: -2575284\n",
      "INFO:lda:<210> log likelihood: -2573530\n",
      "INFO:lda:<220> log likelihood: -2572887\n",
      "INFO:lda:<230> log likelihood: -2572408\n",
      "INFO:lda:<240> log likelihood: -2572238\n",
      "INFO:lda:<250> log likelihood: -2572751\n",
      "INFO:lda:<260> log likelihood: -2573112\n",
      "INFO:lda:<270> log likelihood: -2572225\n",
      "INFO:lda:<280> log likelihood: -2572666\n",
      "INFO:lda:<290> log likelihood: -2573032\n",
      "INFO:lda:<300> log likelihood: -2572509\n",
      "INFO:lda:<310> log likelihood: -2573091\n",
      "INFO:lda:<320> log likelihood: -2573528\n",
      "INFO:lda:<330> log likelihood: -2573611\n",
      "INFO:lda:<340> log likelihood: -2572464\n",
      "INFO:lda:<350> log likelihood: -2573626\n",
      "INFO:lda:<360> log likelihood: -2573266\n",
      "INFO:lda:<370> log likelihood: -2573804\n",
      "INFO:lda:<380> log likelihood: -2570867\n",
      "INFO:lda:<390> log likelihood: -2570054\n",
      "INFO:lda:<400> log likelihood: -2570342\n",
      "INFO:lda:<410> log likelihood: -2571578\n",
      "INFO:lda:<420> log likelihood: -2571531\n",
      "INFO:lda:<430> log likelihood: -2570803\n",
      "INFO:lda:<440> log likelihood: -2571711\n",
      "INFO:lda:<450> log likelihood: -2571981\n",
      "INFO:lda:<460> log likelihood: -2569934\n",
      "INFO:lda:<470> log likelihood: -2570709\n",
      "INFO:lda:<480> log likelihood: -2570437\n",
      "INFO:lda:<490> log likelihood: -2569720\n",
      "INFO:lda:<499> log likelihood: -2570797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: sir come good well go\n",
      "Topic 1: first th shall know must\n",
      "Topic 2: love eye shall one sweet\n",
      "Topic 3: shall hath god good us\n",
      "Topic 4: thou thi thee enter come\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=num_topics, n_iter=num_iter)\n",
    "model.fit(data)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "## Better preprocessing\n",
    "\n",
    "* Look for more lines to remove while parsing corpus (e.g. Enter [NAME], ...)\n",
    "* Downweight words by frequency? tf-idf for example?\n",
    "* Find better stemmer\n",
    "\n",
    "\n",
    "## General directions\n",
    "\n",
    "* Compare stage direction and setting vs. topics from dialogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
