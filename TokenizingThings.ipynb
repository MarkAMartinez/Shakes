{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import parse_corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"preprocessed_shakespeare.txt\"\n",
    "\n",
    "sections = parse_corpus.process(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"/Users/markmartinez/Desktop/shakespeare.txt\",\"r\") as F:\n",
    "#     allText = F.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords, tokenize, stem (takes about 30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rousillon the count palac enter bertram countess of rousillon helena lafeu black countess in deliv son me i buri second husband bertram and i go madam weep father death anew i must attend majesti command i ward evermor subject lafeu you shall find king husband madam you sir father he gener time good must necess hold virtu you whose worthi would stir want rather lack abund countess what hope majesti amend lafeu he hath abandon physician madam whose practic hath persecut time hope find advantag pr\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stopdict = dict((s,None) for s in stops) # Sets are really terrible in Python\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "clean_sections = []\n",
    "\n",
    "# NOTE(tfs): I am using tokenization now. Mainly it really cleans up punctuation and handles contractions well\n",
    "for section in sections:\n",
    "    secwords = section.split()\n",
    "    nonstops = [w.lower() for w in secwords if not w in stopdict]\n",
    "    tokens = nltk.word_tokenize(\" \".join(nonstops))\n",
    "    clean_sections.append(\" \".join([ps.stem(t) for t in tokens if t.isalnum()])) # Note this does not preserve structure,\n",
    "                                              #      but all words are now present in the section string\n",
    "print clean_sections[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stops = set(stopwords.words(\"english\"))\n",
    "# words = allText.split()\n",
    "# meaningful_words = [w for w in words if not w in stops]\n",
    "# cleanedUp =  \" \".join( meaningful_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanedUp[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now split on 'scene'\n",
    "# scenes = [x.strip() for x in cleanedUp.split(\"scene\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "data = vectorizer.fit_transform(clean_sections)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = data.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print sum(train_data_features[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = vectorizer.get_feature_names()\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(data, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run LDA (takes a bit of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "num_topics = 5\n",
    "num_iter = 500\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 643\n",
      "INFO:lda:vocab_size: 13409\n",
      "INFO:lda:n_words: 466303\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -4388122\n",
      "INFO:lda:<10> log likelihood: -4007528\n",
      "INFO:lda:<20> log likelihood: -3900993\n",
      "INFO:lda:<30> log likelihood: -3859116\n",
      "INFO:lda:<40> log likelihood: -3837217\n",
      "INFO:lda:<50> log likelihood: -3818231\n",
      "INFO:lda:<60> log likelihood: -3806042\n",
      "INFO:lda:<70> log likelihood: -3797348\n",
      "INFO:lda:<80> log likelihood: -3790883\n",
      "INFO:lda:<90> log likelihood: -3783404\n",
      "INFO:lda:<100> log likelihood: -3776745\n",
      "INFO:lda:<110> log likelihood: -3771657\n",
      "INFO:lda:<120> log likelihood: -3769947\n",
      "INFO:lda:<130> log likelihood: -3766477\n",
      "INFO:lda:<140> log likelihood: -3763768\n",
      "INFO:lda:<150> log likelihood: -3759820\n",
      "INFO:lda:<160> log likelihood: -3756457\n",
      "INFO:lda:<170> log likelihood: -3754123\n",
      "INFO:lda:<180> log likelihood: -3752625\n",
      "INFO:lda:<190> log likelihood: -3749106\n",
      "INFO:lda:<200> log likelihood: -3748932\n",
      "INFO:lda:<210> log likelihood: -3748789\n",
      "INFO:lda:<220> log likelihood: -3747641\n",
      "INFO:lda:<230> log likelihood: -3748265\n",
      "INFO:lda:<240> log likelihood: -3746792\n",
      "INFO:lda:<250> log likelihood: -3745565\n",
      "INFO:lda:<260> log likelihood: -3745145\n",
      "INFO:lda:<270> log likelihood: -3743577\n",
      "INFO:lda:<280> log likelihood: -3740935\n",
      "INFO:lda:<290> log likelihood: -3740116\n",
      "INFO:lda:<300> log likelihood: -3738989\n",
      "INFO:lda:<310> log likelihood: -3740755\n",
      "INFO:lda:<320> log likelihood: -3739870\n",
      "INFO:lda:<330> log likelihood: -3738076\n",
      "INFO:lda:<340> log likelihood: -3737617\n",
      "INFO:lda:<350> log likelihood: -3737052\n",
      "INFO:lda:<360> log likelihood: -3735227\n",
      "INFO:lda:<370> log likelihood: -3735030\n",
      "INFO:lda:<380> log likelihood: -3733441\n",
      "INFO:lda:<390> log likelihood: -3734069\n",
      "INFO:lda:<400> log likelihood: -3732135\n",
      "INFO:lda:<410> log likelihood: -3732501\n",
      "INFO:lda:<420> log likelihood: -3732334\n",
      "INFO:lda:<430> log likelihood: -3731418\n",
      "INFO:lda:<440> log likelihood: -3729557\n",
      "INFO:lda:<450> log likelihood: -3729354\n",
      "INFO:lda:<460> log likelihood: -3728007\n",
      "INFO:lda:<470> log likelihood: -3728571\n",
      "INFO:lda:<480> log likelihood: -3726770\n",
      "INFO:lda:<490> log likelihood: -3727744\n",
      "INFO:lda:<499> log likelihood: -3726588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: you lord the to shall that it him first he\n",
      "Topic 1: thi thou and thee of the that me come upon\n",
      "Topic 2: sir caesar you antoni come good master falstaff page shall\n",
      "Topic 3: king and lord henri queen shall gloucest richard york princ\n",
      "Topic 4: love you and good come well it go that me\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=num_topics, n_iter=num_iter)\n",
    "model.fit(data)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
