{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import parse_corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"preprocessed_shakespeare.txt\"\n",
    "castfile = \"CleanedCast.txt\"\n",
    "\n",
    "sections = parse_corpus.process(filename)\n",
    "castdict = {}\n",
    "for line in open(castfile):\n",
    "    words = nltk.word_tokenize(line.strip())\n",
    "    for word in words:\n",
    "        castdict[word] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"/Users/markmartinez/Desktop/shakespeare.txt\",\"r\") as F:\n",
    "#     allText = F.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords, tokenize, stem (takes about 30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rousillon the count palac enter bertram countess of rousillon helena lafeu black countess in deliv i buri husband bertram and i go madam weep death anew i must attend majesti command i ward evermor subject lafeu you shall find king husband madam he gener time good must necess hold virtu whose worthi would stir want rather lack abund countess what hope majesti amend lafeu he hath abandon physician madam whose practic hath persecut hope find advantag process lose hope countess thi o sad passag ski\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stopdict = dict((s,None) for s in stops) # Sets are really terrible in Python\n",
    "# print [s for s in stopdict.iterkeys()]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "clean_sections = []\n",
    "\n",
    "# NOTE(tfs): I am using tokenization now. Mainly it really cleans up punctuation and handles contractions well\n",
    "for section in sections:\n",
    "#     secwords = section.split()\n",
    "#     tokens = nltk.word_tokenize(\" \".join(secwords))\n",
    "    tokens = nltk.word_tokenize(section)\n",
    "    nonstops = [w for w in tokens if not (w in stopdict or w in castdict)]\n",
    "    stemmed = [ps.stem(t.lower()) for t in nonstops if t.isalnum()]\n",
    "#     nonstops = [w for w in stemmed if not (w in stopdict or w in castdict)]\n",
    "    clean_sections.append(\" \".join(stemmed)) # Note this does not preserve structure,\n",
    "                                              #      but all words are now present in the section string\n",
    "print clean_sections[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stops = set(stopwords.words(\"english\"))\n",
    "# words = allText.split()\n",
    "# meaningful_words = [w for w in words if not w in stops]\n",
    "# cleanedUp =  \" \".join( meaningful_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanedUp[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now split on 'scene'\n",
    "# scenes = [x.strip() for x in cleanedUp.split(\"scene\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "data = vectorizer.fit_transform(clean_sections)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = data.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n",
      "13388\n",
      "[u'10', u'aaron', u'abaissiez', u'abandon', u'abas', u'abash', u'abat', u'abbess', u'abbey', u'abbot', u'abbrevi', u'abe', u'abel', u'abergavenni', u'abet', u'abhor', u'abhorr', u'abhorson', u'abi', u'abid', u'abil', u'abject', u'abjectli', u'abjur', u'abl', u'abler', u'aboard', u'abod', u'abomin', u'abort', u'abound', u'about', u'abov', u'abr', u'abraham', u'abram', u'abreast', u'abridg', u'abroach', u'abroad', u'abrog', u'abrook', u'abrupt', u'abruptli', u'absenc', u'absent', u'absey', u'absolut', u'absolv', u'abstain', u'abstemi', u'abstin', u'abstract', u'absurd', u'absyrtu', u'abu', u'abund', u'abundantli', u'abus', u'abysm', u'academ', u'accent', u'accept', u'access', u'accessari', u'accid', u'accident', u'accit', u'acclam', u'accommod', u'accommodo', u'accompani', u'accomplic', u'accomplish', u'accompt', u'accord', u'accordeth', u'accordingli', u'accost', u'account', u'accoutr', u'accru', u'accu', u'accumul', u'accur', u'accurs', u'accurst', u'accus', u'accusativo', u'accuseth', u'accustom', u'ace', u'acerb', u'ach', u'acheron', u'achiev', u'achil', u'achitophel', u'acknowledg', u'acknown']\n"
     ]
    }
   ],
   "source": [
    "print sum(train_data_features[300])\n",
    "print len(vocab)\n",
    "print vocab[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# vocab = vectorizer.get_feature_names()\n",
    "# print(len(vocab))\n",
    "\n",
    "print type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "\n",
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(data, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run LDA (takes a bit of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "num_topics = 20\n",
    "num_iter = 500\n",
    "n_top_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 643\n",
      "INFO:lda:vocab_size: 13388\n",
      "INFO:lda:n_words: 420044\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -4754734\n",
      "INFO:lda:<10> log likelihood: -3814275\n",
      "INFO:lda:<20> log likelihood: -3671482\n",
      "INFO:lda:<30> log likelihood: -3612177\n",
      "INFO:lda:<40> log likelihood: -3571192\n",
      "INFO:lda:<50> log likelihood: -3541326\n",
      "INFO:lda:<60> log likelihood: -3519550\n",
      "INFO:lda:<70> log likelihood: -3497999\n",
      "INFO:lda:<80> log likelihood: -3481025\n",
      "INFO:lda:<90> log likelihood: -3466987\n",
      "INFO:lda:<100> log likelihood: -3457343\n",
      "INFO:lda:<110> log likelihood: -3446237\n",
      "INFO:lda:<120> log likelihood: -3436638\n",
      "INFO:lda:<130> log likelihood: -3429910\n",
      "INFO:lda:<140> log likelihood: -3420782\n",
      "INFO:lda:<150> log likelihood: -3413559\n",
      "INFO:lda:<160> log likelihood: -3407148\n",
      "INFO:lda:<170> log likelihood: -3400221\n",
      "INFO:lda:<180> log likelihood: -3396379\n",
      "INFO:lda:<190> log likelihood: -3392249\n",
      "INFO:lda:<200> log likelihood: -3387713\n",
      "INFO:lda:<210> log likelihood: -3384276\n",
      "INFO:lda:<220> log likelihood: -3380221\n",
      "INFO:lda:<230> log likelihood: -3376166\n",
      "INFO:lda:<240> log likelihood: -3371878\n",
      "INFO:lda:<250> log likelihood: -3370594\n",
      "INFO:lda:<260> log likelihood: -3364241\n",
      "INFO:lda:<270> log likelihood: -3359183\n",
      "INFO:lda:<280> log likelihood: -3354707\n",
      "INFO:lda:<290> log likelihood: -3351816\n",
      "INFO:lda:<300> log likelihood: -3347630\n",
      "INFO:lda:<310> log likelihood: -3344314\n",
      "INFO:lda:<320> log likelihood: -3341921\n",
      "INFO:lda:<330> log likelihood: -3339774\n",
      "INFO:lda:<340> log likelihood: -3336686\n",
      "INFO:lda:<350> log likelihood: -3332867\n",
      "INFO:lda:<360> log likelihood: -3329232\n",
      "INFO:lda:<370> log likelihood: -3326959\n",
      "INFO:lda:<380> log likelihood: -3323874\n",
      "INFO:lda:<390> log likelihood: -3323097\n",
      "INFO:lda:<400> log likelihood: -3320623\n",
      "INFO:lda:<410> log likelihood: -3318446\n",
      "INFO:lda:<420> log likelihood: -3318038\n",
      "INFO:lda:<430> log likelihood: -3319537\n",
      "INFO:lda:<440> log likelihood: -3317504\n",
      "INFO:lda:<450> log likelihood: -3317064\n",
      "INFO:lda:<460> log likelihood: -3313653\n",
      "INFO:lda:<470> log likelihood: -3312955\n",
      "INFO:lda:<480> log likelihood: -3312226\n",
      "INFO:lda:<490> log likelihood: -3311980\n",
      "INFO:lda:<499> log likelihood: -3309850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: rosalind helena love parol orlando\n",
      "Topic 1: sir falstaff page ford master\n",
      "Topic 2: valentin proteu berown love julia\n",
      "Topic 3: the of great enter macbeth\n",
      "Topic 4: duke clown angelo olivia isabella\n",
      "Topic 5: timon petruchio servant tranio and\n",
      "Topic 6: portia antonio bassanio shylock ring\n",
      "Topic 7: and titu thi son rome\n",
      "Topic 8: imogen prospero th posthumu cymbelin\n",
      "Topic 9: king gloucest queen york richard\n",
      "Topic 10: enter and talbot fight the\n",
      "Topic 11: king and franc princ shall\n",
      "Topic 12: second first coriolanu citizen meneniu\n",
      "Topic 13: thou thi thee art hast\n",
      "Topic 14: iago othello cassio desdemona gentleman\n",
      "Topic 15: to and that the my\n",
      "Topic 16: come good well shall would\n",
      "Topic 17: caesar antoni brutu cleopatra cassiu\n",
      "Topic 18: of troilu syracus dromio antipholu\n",
      "Topic 19: and love thi that the\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=num_topics, n_iter=num_iter)\n",
    "model.fit(data)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
