{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import parse_corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"preprocessed_shakespeare.txt\"\n",
    "castfile = \"curated_cast.txt\"\n",
    "\n",
    "sections = parse_corpus.process(filename)\n",
    "castdict = {}\n",
    "for line in open(castfile):\n",
    "    words = nltk.word_tokenize(line.strip())\n",
    "    for word in words:\n",
    "        castdict[word.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"/Users/markmartinez/Desktop/shakespeare.txt\",\"r\") as F:\n",
    "#     allText = F.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords, tokenize, stem (takes about 30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palac enter black deliv buri husband go madam weep death anew must attend majesti command ward evermor subject shall find husband madam sir gener time good must necess hold virtu whose worthi would stir want rather lack abund hope majesti amend hath abandon physician madam whose practic hath persecut time hope find advantag process lose hope time sad passag skill almost great honesti stretch far would made natur immort death play lack work would sake live think would death diseas call speak mada\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stopdict = dict((s.lower(),None) for s in stops) # Sets are really terrible in Python\n",
    "# print [s for s in stopdict.iterkeys()]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "clean_sections = []\n",
    "\n",
    "# NOTE(tfs): I am using tokenization now. Mainly it really cleans up punctuation and handles contractions well\n",
    "for section in sections:\n",
    "#     secwords = section.split()\n",
    "#     tokens = nltk.word_tokenize(\" \".join(secwords))\n",
    "    tokens = nltk.word_tokenize(section)\n",
    "    nonstops = [w for w in tokens if not (w.lower() in stopdict or w.lower() in castdict)]\n",
    "    stemmed = [ps.stem(t.lower()) for t in nonstops if t.isalnum()]\n",
    "#     nonstops = [w for w in stemmed if not (w in stopdict or w in castdict)]\n",
    "    clean_sections.append(\" \".join(stemmed)) # Note this does not preserve structure,\n",
    "                                              #      but all words are now present in the section string\n",
    "print clean_sections[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stops = set(stopwords.words(\"english\"))\n",
    "# words = allText.split()\n",
    "# meaningful_words = [w for w in words if not w in stops]\n",
    "# cleanedUp =  \" \".join( meaningful_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanedUp[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now split on 'scene'\n",
    "# scenes = [x.strip() for x in cleanedUp.split(\"scene\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "data = vectorizer.fit_transform(clean_sections)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = data.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402\n",
      "12740\n",
      "[u'acquit', u'acquitt', u'acr', u'across', u'act', u'actaeon', u'action', u'actium', u'activ', u'actor', u'actual', u'acut', u'ad', u'adag', u'adalla', u'adam', u'add', u'adder', u'addict', u'addit', u'addl', u'address', u'addrest', u'adher', u'adieu', u'adjac', u'adjoin', u'adjourn', u'adjudg', u'adjunct', u'administ', u'administr', u'admir', u'admiringli', u'admit', u'admitt', u'admonish', u'admonit', u'ado', u'adoni', u'adopt', u'adoptedli', u'adopti', u'ador', u'adorest', u'adoreth', u'adorn', u'adramadio', u'adriat', u'adsum', u'adul', u'adulter', u'adulteress', u'adulteri', u'adultress', u'advanc', u'advantag', u'adventur', u'advers', u'adversari', u'adverti', u'advertis', u'advi', u'advic', u'advis', u'advisedli', u'advoc', u'aeacid', u'aeacida', u'aedil', u'aegl', u'aeolu', u'aer', u'aeri', u'aerial', u'aesculapiu', u'aeson', u'aesop', u'aetna', u'afar', u'afear', u'afeard', u'affabl', u'affair', u'affect', u'affecteth', u'affection', u'affeer', u'affi', u'affianc', u'affin', u'affirm', u'afflict', u'afford', u'affordeth', u'affright', u'affront', u'afield', u'afir', u'afloat', u'afoot', u'afor', u'aforehand', u'aforesaid', u'afraid', u'afresh', u'afric', u'africa', u'african', u'afternoon', u'afterward', u'ag', u'agamemmon', u'agat', u'agaz', u'age', u'agenor', u'agent', u'aggrav', u'aggrief', u'agil', u'agincourt', u'agit', u'agniz', u'ago', u'agon', u'agoni', u'agre', u'agreement', u'aground', u'agu', u'aguefac', u'ah', u'ahungri', u'ai', u'aialvolio', u'aiaria', u'aid', u'aidanc', u'aidless', u'ail', u'aim', u'aimest', u'ainsi', u'air', u'airi', u'airless', u'akil', u'ala', u'alabast', u'alack', u'alacr', u'alarm', u'alarum', u'alb', u'alban', u'albeit', u'albion', u'alchemi', u'alchemist', u'alcid', u'aldermen', u'ale', u'alecto', u'alehous', u'alengon', u'aleppo', u'alewif', u'alexand', u'alexandria', u'alexandrian', u'alia', u'alic', u'alien', u'aliena', u'alight', u'alik', u'alisand', u'aliv', u'alla', u'allay', u'alleg', u'allegi', u'alley', u'allhallowma', u'alli', u'allianc', u'allicholi', u'allig', u'allon', u'allot', u'allotteri', u'allow', u'allur', u'allus', u'allycholli', u'alm', u'almain', u'almanac', u'almanack']\n"
     ]
    }
   ],
   "source": [
    "print sum(train_data_features[300])\n",
    "print len(vocab)\n",
    "print vocab[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# vocab = vectorizer.get_feature_names()\n",
    "# print(len(vocab))\n",
    "\n",
    "print type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "\n",
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(data, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run LDA (takes a bit of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "num_topics = 20\n",
    "num_iter = 500\n",
    "n_top_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 643\n",
      "INFO:lda:vocab_size: 12740\n",
      "INFO:lda:n_words: 317704\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -3615314\n",
      "INFO:lda:<10> log likelihood: -2962327\n",
      "INFO:lda:<20> log likelihood: -2843815\n",
      "INFO:lda:<30> log likelihood: -2793382\n",
      "INFO:lda:<40> log likelihood: -2765225\n",
      "INFO:lda:<50> log likelihood: -2748100\n",
      "INFO:lda:<60> log likelihood: -2735338\n",
      "INFO:lda:<70> log likelihood: -2723951\n",
      "INFO:lda:<80> log likelihood: -2714338\n",
      "INFO:lda:<90> log likelihood: -2706120\n",
      "INFO:lda:<100> log likelihood: -2701096\n",
      "INFO:lda:<110> log likelihood: -2694974\n",
      "INFO:lda:<120> log likelihood: -2691444\n",
      "INFO:lda:<130> log likelihood: -2687876\n",
      "INFO:lda:<140> log likelihood: -2683818\n",
      "INFO:lda:<150> log likelihood: -2680089\n",
      "INFO:lda:<160> log likelihood: -2676093\n",
      "INFO:lda:<170> log likelihood: -2674596\n",
      "INFO:lda:<180> log likelihood: -2669693\n",
      "INFO:lda:<190> log likelihood: -2666825\n",
      "INFO:lda:<200> log likelihood: -2664069\n",
      "INFO:lda:<210> log likelihood: -2662800\n",
      "INFO:lda:<220> log likelihood: -2662720\n",
      "INFO:lda:<230> log likelihood: -2660757\n",
      "INFO:lda:<240> log likelihood: -2659513\n",
      "INFO:lda:<250> log likelihood: -2658556\n",
      "INFO:lda:<260> log likelihood: -2657071\n",
      "INFO:lda:<270> log likelihood: -2655520\n",
      "INFO:lda:<280> log likelihood: -2653548\n",
      "INFO:lda:<290> log likelihood: -2650768\n",
      "INFO:lda:<300> log likelihood: -2650692\n",
      "INFO:lda:<310> log likelihood: -2646590\n",
      "INFO:lda:<320> log likelihood: -2646316\n",
      "INFO:lda:<330> log likelihood: -2643555\n",
      "INFO:lda:<340> log likelihood: -2641739\n",
      "INFO:lda:<350> log likelihood: -2641596\n",
      "INFO:lda:<360> log likelihood: -2641003\n",
      "INFO:lda:<370> log likelihood: -2641652\n",
      "INFO:lda:<380> log likelihood: -2642761\n",
      "INFO:lda:<390> log likelihood: -2641077\n",
      "INFO:lda:<400> log likelihood: -2638470\n",
      "INFO:lda:<410> log likelihood: -2636354\n",
      "INFO:lda:<420> log likelihood: -2634102\n",
      "INFO:lda:<430> log likelihood: -2633615\n",
      "INFO:lda:<440> log likelihood: -2634453\n",
      "INFO:lda:<450> log likelihood: -2632795\n",
      "INFO:lda:<460> log likelihood: -2631809\n",
      "INFO:lda:<470> log likelihood: -2631227\n",
      "INFO:lda:<480> log likelihood: -2629080\n",
      "INFO:lda:<490> log likelihood: -2628331\n",
      "INFO:lda:<499> log likelihood: -2628265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: rome first coriolanu nobl third\n",
      "Topic 1: enter us shall fight soldier\n",
      "Topic 2: thou thi thee art dost\n",
      "Topic 3: thi thou thee life death\n",
      "Topic 4: kate fluellen katherin william alic\n",
      "Topic 5: come let may shall must\n",
      "Topic 6: sir hath well signior thee\n",
      "Topic 7: sing sweet play enter night\n",
      "Topic 8: murder tell go kiss troy\n",
      "Topic 9: come sleep go enter good\n",
      "Topic 10: england crown unto peac men\n",
      "Topic 11: shall let hath come us\n",
      "Topic 12: th sir us made follow\n",
      "Topic 13: first hath life natur good\n",
      "Topic 14: sister ham law hold mad\n",
      "Topic 15: love shall fair see madam\n",
      "Topic 16: grace god high brother day\n",
      "Topic 17: sir come good go shall\n",
      "Topic 18: like may god hot cousin\n",
      "Topic 19: know make would yet one\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=num_topics, n_iter=num_iter)\n",
    "model.fit(data)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Look for more lines to remove while parsing corpus (e.g. Enter [NAME], ...)\n",
    "* Downweight words by frequency? tf-idf for example?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
