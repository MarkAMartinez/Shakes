{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import parse_corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"preprocessed_shakespeare.txt\"\n",
    "\n",
    "sections = parse_corpus.process(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"/Users/markmartinez/Desktop/shakespeare.txt\",\"r\") as F:\n",
    "#     allText = F.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords, tokenize, stem (takes about 30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rousillon count palac enter bertram countess rousillon helena lafeu black countess deliv son buri second husband bertram go madam weep father death anew must attend hi majesti command ward evermor subject lafeu shall find king husband madam sir father gener time good must necess hold hi virtu whose worthi would stir want rather lack abund countess hope hi majesti amend lafeu hath abandon hi physician madam whose practic hath persecut time hope find advantag process onli lose hope time countess t\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stopdict = dict((s,None) for s in stops) # Sets are really terrible in Python\n",
    "# print [s for s in stopdict.iterkeys()]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "clean_sections = []\n",
    "\n",
    "# NOTE(tfs): I am using tokenization now. Mainly it really cleans up punctuation and handles contractions well\n",
    "for section in sections:\n",
    "#     secwords = section.split()\n",
    "#     tokens = nltk.word_tokenize(\" \".join(secwords))\n",
    "    tokens = nltk.word_tokenize(section)\n",
    "    stemmed = [ps.stem(t.lower()) for t in tokens if t.isalnum()]\n",
    "    nonstops = [w for w in stemmed if not w in stopdict]\n",
    "    clean_sections.append(\" \".join(nonstops)) # Note this does not preserve structure,\n",
    "                                              #      but all words are now present in the section string\n",
    "print clean_sections[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stops = set(stopwords.words(\"english\"))\n",
    "# words = allText.split()\n",
    "# meaningful_words = [w for w in words if not w in stops]\n",
    "# cleanedUp =  \" \".join( meaningful_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanedUp[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now split on 'scene'\n",
    "# scenes = [x.strip() for x in cleanedUp.split(\"scene\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "data = vectorizer.fit_transform(clean_sections)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = data.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "13292\n",
      "[u'10', u'aaron', u'abaissiez', u'abandon', u'abas', u'abash', u'abat', u'abbess', u'abbey', u'abbot', u'abbrevi', u'abe', u'abel', u'abergavenni', u'abet', u'abhor', u'abhorr', u'abhorson', u'abi', u'abid', u'abil', u'abject', u'abjectli', u'abjur', u'abl', u'abler', u'aboard', u'abod', u'abomin', u'abort', u'abound', u'abov', u'abr', u'abraham', u'abram', u'abreast', u'abridg', u'abroach', u'abroad', u'abrog', u'abrook', u'abrupt', u'abruptli', u'absenc', u'absent', u'absey', u'absolut', u'absolv', u'abstain', u'abstemi', u'abstin', u'abstract', u'absurd', u'absyrtu', u'abu', u'abund', u'abundantli', u'abus', u'abysm', u'academ', u'accent', u'accept', u'access', u'accessari', u'accid', u'accident', u'accit', u'acclam', u'accommod', u'accommodo', u'accompani', u'accomplic', u'accomplish', u'accompt', u'accord', u'accordeth', u'accordingli', u'accost', u'account', u'accoutr', u'accru', u'accu', u'accumul', u'accur', u'accurs', u'accurst', u'accus', u'accusativo', u'accuseth', u'accustom', u'ace', u'acerb', u'ach', u'acheron', u'achiev', u'achil', u'achitophel', u'acknowledg', u'acknown', u'acold']\n"
     ]
    }
   ],
   "source": [
    "print sum(train_data_features[300])\n",
    "print len(vocab)\n",
    "print vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# vocab = vectorizer.get_feature_names()\n",
    "# print(len(vocab))\n",
    "\n",
    "print type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "\n",
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(data, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run LDA (takes a bit of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "num_topics = 20\n",
    "num_iter = 500\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 643\n",
      "INFO:lda:vocab_size: 13292\n",
      "INFO:lda:n_words: 400417\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -4537203\n",
      "INFO:lda:<10> log likelihood: -3628801\n",
      "INFO:lda:<20> log likelihood: -3483210\n",
      "INFO:lda:<30> log likelihood: -3426996\n",
      "INFO:lda:<40> log likelihood: -3390218\n",
      "INFO:lda:<50> log likelihood: -3362310\n",
      "INFO:lda:<60> log likelihood: -3333776\n",
      "INFO:lda:<70> log likelihood: -3312367\n",
      "INFO:lda:<80> log likelihood: -3291160\n",
      "INFO:lda:<90> log likelihood: -3271056\n",
      "INFO:lda:<100> log likelihood: -3253946\n",
      "INFO:lda:<110> log likelihood: -3236832\n",
      "INFO:lda:<120> log likelihood: -3225059\n",
      "INFO:lda:<130> log likelihood: -3213798\n",
      "INFO:lda:<140> log likelihood: -3205105\n",
      "INFO:lda:<150> log likelihood: -3198618\n",
      "INFO:lda:<160> log likelihood: -3188489\n",
      "INFO:lda:<170> log likelihood: -3178922\n",
      "INFO:lda:<180> log likelihood: -3173257\n",
      "INFO:lda:<190> log likelihood: -3164682\n",
      "INFO:lda:<200> log likelihood: -3155372\n",
      "INFO:lda:<210> log likelihood: -3150289\n",
      "INFO:lda:<220> log likelihood: -3145589\n",
      "INFO:lda:<230> log likelihood: -3143537\n",
      "INFO:lda:<240> log likelihood: -3139462\n",
      "INFO:lda:<250> log likelihood: -3137242\n",
      "INFO:lda:<260> log likelihood: -3135613\n",
      "INFO:lda:<270> log likelihood: -3132904\n",
      "INFO:lda:<280> log likelihood: -3131036\n",
      "INFO:lda:<290> log likelihood: -3127679\n",
      "INFO:lda:<300> log likelihood: -3124883\n",
      "INFO:lda:<310> log likelihood: -3125030\n",
      "INFO:lda:<320> log likelihood: -3121959\n",
      "INFO:lda:<330> log likelihood: -3122555\n",
      "INFO:lda:<340> log likelihood: -3120008\n",
      "INFO:lda:<350> log likelihood: -3118371\n",
      "INFO:lda:<360> log likelihood: -3117794\n",
      "INFO:lda:<370> log likelihood: -3117745\n",
      "INFO:lda:<380> log likelihood: -3117962\n",
      "INFO:lda:<390> log likelihood: -3117658\n",
      "INFO:lda:<400> log likelihood: -3117412\n",
      "INFO:lda:<410> log likelihood: -3116168\n",
      "INFO:lda:<420> log likelihood: -3115973\n",
      "INFO:lda:<430> log likelihood: -3117567\n",
      "INFO:lda:<440> log likelihood: -3114999\n",
      "INFO:lda:<450> log likelihood: -3114400\n",
      "INFO:lda:<460> log likelihood: -3112902\n",
      "INFO:lda:<470> log likelihood: -3112080\n",
      "INFO:lda:<480> log likelihood: -3113436\n",
      "INFO:lda:<490> log likelihood: -3112012\n",
      "INFO:lda:<499> log likelihood: -3111833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hi lord us honour first great hath enter exeunt way\n",
      "Topic 1: king gloucest lord thi henri warwick hi edward duke suffolk\n",
      "Topic 2: titu luciu rome marcu princ bardolph thi aaron hostess tamora\n",
      "Topic 3: antonio sebastian pedro beat bene leon claud hero gonzalo ladi\n",
      "Topic 4: love hi wa one would man know whi say hath\n",
      "Topic 5: caesar antoni brutu cleopatra cassiu enobarbu charmian pompey come casca\n",
      "Topic 6: macbeth ladi macduff banquo ross witch murther malcolm time ham\n",
      "Topic 7: berown king love princess armado costard franc moth sweet demetriu\n",
      "Topic 8: syracus dromio antipholu ephesu sir prospero ariel adriana stephano thou\n",
      "Topic 9: thi shall let come make may well good must yet\n",
      "Topic 10: sir page falstaff master ford come good shallow mistress john\n",
      "Topic 11: sir clown tobi thi parol imogen olivia ladi viola th\n",
      "Topic 12: thou thi thee art enter come hast thine upon dost\n",
      "Topic 13: king queen richard thi son death father thou york die\n",
      "Topic 14: petruchio portia sir tranio lucentio bassanio hortensio baptista katherina shylock\n",
      "Topic 15: timon troilu servant pandaru lord cressida apemantu leont camillo th\n",
      "Topic 16: duke rosalind valentin sir proteu love angelo orlando isabella julia\n",
      "Topic 17: iago othello cassio desdemona achil emilia thersit ajax ulyss roderigo\n",
      "Topic 18: first second citizen coriolanu meneniu th third rome marciu siciniu\n",
      "Topic 19: franc talbot french charl henri king dauphin soldier fluellen english\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=num_topics, n_iter=num_iter)\n",
    "model.fit(data)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
